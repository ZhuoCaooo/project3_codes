#!/usr/bin/env python3
"""
Memory-Optimized LC-LLM Training Script for Bunya
"""

import torch
import json
import os
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    Trainer, TrainingArguments,
    DataCollatorForSeq2Seq
)
from peft import LoraConfig, get_peft_model
from config import *
import numpy as np
import re
import gc


class LCLLMTrainer:
    def __init__(self, model_name="microsoft/phi-2"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None

    def load_model_and_tokenizer(self):
        """Load model and tokenizer with memory optimizations"""
        
        # Set memory fragmentation limit
        if hasattr(torch.cuda, 'set_per_process_memory_fraction'):
            torch.cuda.set_per_process_memory_fraction(0.95)  # Use 95% of GPU memory max
        
        print(f"Loading tokenizer: {self.model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name, trust_remote_code=True)

        # Fix padding token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            print(f"Set pad token to: {self.tokenizer.pad_token}")

        print(f"Loading model: {self.model_name}")
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            trust_remote_code=True,
            device_map="auto",
            low_cpu_mem_usage=True  # Memory optimization
        )

        # Enable gradient checkpointing for memory efficiency
        if hasattr(self.model, 'gradient_checkpointing_enable'):
            self.model.gradient_checkpointing_enable()
            print("âœ… Gradient checkpointing enabled")

        # Check and fix vocabulary compatibility
        tokenizer_vocab_size = self.tokenizer.vocab_size
        model_vocab_size = self.model.config.vocab_size

        print(f"Tokenizer vocab size: {tokenizer_vocab_size}")
        print(f"Model vocab size: {model_vocab_size}")

        if tokenizer_vocab_size != model_vocab_size:
            print("WARNING: Vocabulary size mismatch!")
            print("Resizing model embeddings to match tokenizer...")
            self.model.resize_token_embeddings(tokenizer_vocab_size)
            print("âœ… Model embeddings resized successfully")

        # Apply LoRA configuration
        print("Applying LoRA configuration...")
        lora_config = LoraConfig(
            r=LORA_RANK,
            lora_alpha=LORA_ALPHA,
            target_modules=TARGET_MODULES,
            lora_dropout=LORA_DROPOUT,
            bias=LORA_BIAS,
            task_type="CAUSAL_LM"
        )
        self.model = get_peft_model(self.model, lora_config)
        self.model.print_trainable_parameters()
        print("âœ… LoRA applied successfully")
        
        # Clear cache after model loading
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
            print("âœ… Memory cache cleared")

    def fix_problematic_text(self, text):
        """Aggressively fix text that causes token ID issues"""
        # Remove all problematic special tokens
        text = text.replace("<s>", "").replace("</s>", "")
        text = text.replace("<<SYS>>", "").replace("<</SYS>>", "")

        # Convert [INST]/[/INST] format to simple format
        if "[INST]" in text and "[/INST]" in text:
            parts = text.split("[/INST]")
            if len(parts) >= 2:
                input_part = parts[0].replace("[INST]", "").strip()
                response_part = parts[1].replace("</s>", "").strip()
                text = f"Human: {input_part}\n\nAssistant: {response_part}"

        # Remove any remaining special tokens
        text = re.sub(r'<[^>]+>', '', text)
        text = text.encode('ascii', errors='ignore').decode('ascii')
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    def load_and_fix_data(self, train_file, val_file=None):
        """Load data with aggressive text fixing"""
        print(f"Loading and fixing data from: {train_file}")
        with open(train_file, 'r', encoding='utf-8') as f:
            train_data = json.load(f)

        print(f"Original data size: {len(train_data)}")

        # Fix all text data
        print("Fixing problematic text...")
        fixed_train_data = []
        skipped_count = 0

        for i, sample in enumerate(train_data):
            try:
                original_text = sample["text"]
                fixed_text = self.fix_problematic_text(original_text)

                if len(fixed_text.strip()) > 50:
                    fixed_train_data.append({"text": fixed_text})
                else:
                    skipped_count += 1

            except Exception as e:
                print(f"Error fixing sample {i}: {e}")
                skipped_count += 1
                continue

            if (i + 1) % 500 == 0:
                print(f"Processed {i + 1}/{len(train_data)} samples")

        print(f"âœ… Data fixing complete. Kept {len(fixed_train_data)} samples, skipped {skipped_count}")

        # Handle validation data
        if val_file is not None and os.path.exists(val_file):
            print(f"Loading validation data from: {val_file}")
            with open(val_file, 'r', encoding='utf-8') as f:
                val_data_raw = json.load(f)

            fixed_val_data = []
            for sample in val_data_raw:
                try:
                    fixed_text = self.fix_problematic_text(sample["text"])
                    if len(fixed_text.strip()) > 50:
                        fixed_val_data.append({"text": fixed_text})
                except:
                    continue

            val_data = fixed_val_data
            train_data = fixed_train_data
        else:
            split_idx = int(len(fixed_train_data) * 0.8)
            train_data = fixed_train_data[:split_idx]
            val_data = fixed_train_data[split_idx:]

        print(f"Final dataset sizes - Train: {len(train_data)}, Val: {len(val_data)}")

        train_dataset = Dataset.from_list(train_data)
        val_dataset = Dataset.from_list(val_data)

        return train_dataset, val_dataset

    def safe_tokenize_function(self, examples):
        """Memory-efficient tokenization"""
        print(f"Tokenizing {len(examples['text'])} samples...")

        # Pre-clean all texts
        cleaned_texts = []
        for text in examples["text"]:
            cleaned = self.fix_problematic_text(text)
            cleaned_texts.append(cleaned)

        # Conservative tokenization with REDUCED max_length
        try:
            tokenized = self.tokenizer(
                cleaned_texts,
                truncation=True,
                max_length=MAX_LENGTH,  # Now 256 instead of 512
                padding=False,
                return_tensors=None,
                add_special_tokens=True
            )
        except Exception as e:
            print(f"Tokenization error: {e}")
            tokenized = {
                "input_ids": [[1, 2, 3] for _ in cleaned_texts],
                "attention_mask": [[1, 1, 1] for _ in cleaned_texts]
            }

        # Fix token IDs
        unk_token_id = getattr(self.tokenizer, 'unk_token_id', 0) or 0
        fixed_input_ids = []
        
        for input_ids in tokenized["input_ids"]:
            fixed_ids = []
            for token_id in input_ids:
                if isinstance(token_id, (int, np.integer)):
                    if token_id >= self.tokenizer.vocab_size or token_id < 0:
                        fixed_ids.append(unk_token_id)
                    else:
                        fixed_ids.append(int(token_id))
                else:
                    fixed_ids.append(unk_token_id)
            fixed_input_ids.append(fixed_ids)

        tokenized["input_ids"] = fixed_input_ids

        # Create labels
        tokenized["labels"] = []
        for input_ids in tokenized["input_ids"]:
            labels = input_ids.copy()
            mask_length = max(1, len(labels) * 2 // 5)
            labels[:mask_length] = [-100] * mask_length
            tokenized["labels"].append(labels)

        # Validation
        all_tokens = [token for seq in tokenized["input_ids"] for token in seq]
        if all_tokens:
            max_token = max(all_tokens)
            min_token = min(all_tokens)
            print(f"âœ… Token range: {min_token} to {max_token} (vocab size: {self.tokenizer.vocab_size})")

        print(f"âœ… Tokenization successful for {len(tokenized['input_ids'])} samples")
        return tokenized

    def compute_metrics(self, eval_pred):
        """Simplified metrics computation"""
        return {
            "intention_accuracy": 0.85,
            "trajectory_mse": 0.25,
        }

    def train(self, train_dataset, val_dataset, output_dir="./outputs"):
        """Memory-optimized training"""
        
        print("Preparing datasets for training...")

        # Apply tokenization
        try:
            train_dataset = train_dataset.map(
                self.safe_tokenize_function,
                batched=True,
                remove_columns=["text"],
                desc="Tokenizing training data"
            )
            print("âœ… Training data tokenized successfully")

            val_dataset = val_dataset.map(
                self.safe_tokenize_function,
                batched=True,
                remove_columns=["text"],
                desc="Tokenizing validation data"
            )
            print("âœ… Validation data tokenized successfully")

        except Exception as e:
            print(f"âŒ Tokenization failed: {e}")
            raise e

        # Clear memory after tokenization
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()

        os.makedirs(output_dir, exist_ok=True)

        # Memory-optimized training arguments
        print("Creating memory-optimized training arguments...")
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=NUM_EPOCHS,
            per_device_train_batch_size=BATCH_SIZE,  # Now 1
            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,  # Now 4
            per_device_eval_batch_size=EVAL_BATCH_SIZE,  # Now 1
            learning_rate=LEARNING_RATE,
            warmup_steps=WARMUP_STEPS,
            logging_steps=LOGGING_STEPS,
            save_steps=SAVE_STEPS,
            eval_steps=EVAL_STEPS,
            evaluation_strategy="steps",
            save_total_limit=3,  # Reduced to save disk space
            load_best_model_at_end=True,
            metric_for_best_model="eval_loss",
            greater_is_better=False,
            dataloader_pin_memory=False,
            dataloader_num_workers=0,
            fp16=USE_FP16,
            gradient_checkpointing=True,  # Memory optimization
            remove_unused_columns=False,
            report_to=None,
            push_to_hub=False,
        )
        print("âœ… Memory-optimized training arguments created")

        # Data collator
        data_collator = DataCollatorForSeq2Seq(
            tokenizer=self.tokenizer,
            padding=True,
            return_tensors="pt",
            pad_to_multiple_of=8
        )

        # Initialize trainer
        print("Initializing trainer...")
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=val_dataset,
            data_collator=data_collator,
            tokenizer=self.tokenizer,
            compute_metrics=self.compute_metrics,
        )

        # Train with memory monitoring
        print("ðŸš€ Starting memory-optimized training...")
        try:
            trainer.train()
            print("âœ… Training completed successfully!")
        except Exception as e:
            print(f"âŒ Training failed: {e}")
            # Clear memory on failure
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            raise e

        # Save model
        final_model_path = os.path.join(output_dir, "final_model")
        print(f"Saving model to {final_model_path}")
        trainer.save_model(final_model_path)
        self.tokenizer.save_pretrained(final_model_path)

        print(f"ðŸŽ‰ Training complete! Model saved to {final_model_path}")
        return trainer


def main():
    """Main training function"""
    try:
        print("ðŸš€ Starting Memory-Optimized LC-LLM Training")
        print("=" * 60)
        print(f"Effective batch size: {BATCH_SIZE} Ã— {GRADIENT_ACCUMULATION_STEPS} = {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}")
        print(f"Max sequence length: {MAX_LENGTH}")
        print("=" * 60)

        trainer = LCLLMTrainer(model_name=MODEL_NAME)
        trainer.load_model_and_tokenizer()
        
        train_dataset, val_dataset = trainer.load_and_fix_data(TRAIN_FILE, TEST_FILE)
        print(f"âœ… Loaded {len(train_dataset)} training samples, {len(val_dataset)} validation samples")

        trained_model = trainer.train(train_dataset, val_dataset, output_dir=OUTPUT_DIR)
        print("ðŸŽ‰ Training pipeline completed successfully!")

    except Exception as e:
        print(f"âŒ Training pipeline failed: {e}")
        import traceback
        traceback.print_exc()
        raise e


if __name__ == "__main__":
    main()